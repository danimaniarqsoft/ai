Prepare the data to better expose the underlying data patterns to Machine Learning algorithms

This step is also know as **Preprocessing**

> [!NOTE] Notes
>- Work on copies of the data (keep the original dataset intact)
>- Write functions for all data transformations you apply, for five reasons:
>	- So you can easily prepare the data the next time you get a fresh dataset
>	- So you can apply these transformations in future projects
>	- To clean and prepare new data instance once your solution is live To make it easy to treat you preparations choices as hyperparameters

1. Data cleaning:
	1. Fix or remove outliers (optional)
	2. Fill in missing values (e.g. with zero, mean, median...) or drop their rows (or columns)
		1. If loss data is less or equals to 1% of the total data, then we can remove the data
2. Future selection (optional)
	1. Drop the attributes that provide no useful information for the task
3. Feature engineering, where appropriate:
	1. Discretize continuous features
	2. Decompose features (e.g., categorical, date/time, etc.)
		1. Encoding categorical data
			1. Encoding the independent variable (predictors)
			2. Encoding the dependent variable (What we want to predict)
	3. Add promising transformations of features (e.g. $x^2$, $log(x)$, $\sqrt[2]{x}$, etc)
	4. Aggregate features into promising new features
4. [[Feature scaling]]: standardize or normalize features


> [!NOTE] Training data and Testing data
> Some times, in this step we can separate the training data from the testing data. It is already done in the step **2. Get the data**, but we can also do it in this section.

 Encoding categorical data
 1. One hot encoding technique. It is create a binary vector from the categorical feature. One feature means 0 and 1 binary values, Two features means 00, 01, 10, 11 binaries vectors. Tree features mean 000, 001, 010, 011, 100, 101, 110, 111 binary vectors, and so on.
 2. LabelEncoder technique